[
  {
    "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
    "url": "https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
    "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "citationCount": 43538,
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.01703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "citationStyles": {
      "bibtex": "@Article{Paszke2019PyTorchAI,\n author = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and N. Gimelshein and L. Antiga and Alban Desmaison and Andreas Köpf and E. Yang and Zachary DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},\n volume = {abs/1912.01703},\n year = {2019}\n}\n"
    },
    "authors": [
      {
        "authorId": "3407277",
        "name": "Adam Paszke"
      },
      {
        "authorId": "39793298",
        "name": "Sam Gross"
      },
      {
        "authorId": "1403239967",
        "name": "Francisco Massa"
      },
      {
        "authorId": "1977806",
        "name": "Adam Lerer"
      },
      {
        "authorId": "2065251344",
        "name": "James Bradbury"
      },
      {
        "authorId": "114250963",
        "name": "Gregory Chanan"
      },
      {
        "authorId": "2059271276",
        "name": "Trevor Killeen"
      },
      {
        "authorId": "3370429",
        "name": "Zeming Lin"
      },
      {
        "authorId": "3365851",
        "name": "N. Gimelshein"
      },
      {
        "authorId": "3029482",
        "name": "L. Antiga"
      },
      {
        "authorId": "3050846",
        "name": "Alban Desmaison"
      },
      {
        "authorId": "1473151134",
        "name": "Andreas Köpf"
      },
      {
        "authorId": "2052812305",
        "name": "E. Yang"
      },
      {
        "authorId": "2253681376",
        "name": "Zachary DeVito"
      },
      {
        "authorId": "10707709",
        "name": "Martin Raison"
      },
      {
        "authorId": "41203992",
        "name": "Alykhan Tejani"
      },
      {
        "authorId": "22236100",
        "name": "Sasank Chilamkurthy"
      },
      {
        "authorId": "32163737",
        "name": "Benoit Steiner"
      },
      {
        "authorId": "152599430",
        "name": "Lu Fang"
      },
      {
        "authorId": "2113829116",
        "name": "Junjie Bai"
      },
      {
        "authorId": "2127604",
        "name": "Soumith Chintala"
      }
    ],
    "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",
    "category": "2015–2019"
  }
]